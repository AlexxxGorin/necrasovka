"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞
–í–∫–ª—é—á–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
"""

import math
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from collections import defaultdict

@dataclass
class SearchMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞"""
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    precision_at_k: Dict[int, float]  # P@K –¥–ª—è —Ä–∞–∑–Ω—ã—Ö K
    recall_at_k: Dict[int, float]     # R@K –¥–ª—è —Ä–∞–∑–Ω—ã—Ö K
    ndcg_at_k: Dict[int, float]       # NDCG@K –¥–ª—è —Ä–∞–∑–Ω—ã—Ö K
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    mrr: float                        # Mean Reciprocal Rank
    map_score: float                  # Mean Average Precision
    
    # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    title_match_score: float          # –°–∫–æ—Ä —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö
    diversity_score: float            # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    speed_score: float                # –°–∫–æ—Ä —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    category_scores: Dict[str, float] # –°–∫–æ—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º

class AdvancedSearchEvaluator:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –æ—Ü–µ–Ω—â–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞"""
    
    def __init__(self):
        self.relevance_levels = {
            "perfect": 3,      # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            "excellent": 2,    # –û—á–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
            "good": 1,         # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
            "irrelevant": 0    # –ù–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
        }
    
    def calculate_precision_at_k(self, relevant_items: List[bool], k: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç Precision@K"""
        if k == 0 or len(relevant_items) == 0:
            return 0.0
        
        top_k = relevant_items[:k]
        return sum(top_k) / len(top_k)
    
    def calculate_recall_at_k(self, relevant_items: List[bool], total_relevant: int, k: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç Recall@K"""
        if total_relevant == 0 or k == 0:
            return 0.0
        
        top_k = relevant_items[:k]
        return sum(top_k) / total_relevant
    
    def calculate_ndcg_at_k(self, relevance_scores: List[int], k: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç NDCG@K (Normalized Discounted Cumulative Gain)"""
        if k == 0 or not relevance_scores:
            return 0.0
        
        # DCG@K
        dcg = 0.0
        for i, score in enumerate(relevance_scores[:k]):
            if score > 0:
                dcg += score / math.log2(i + 2)  # +2 –ø–æ—Ç–æ–º—É —á—Ç–æ log2(1) = 0
        
        # IDCG@K (–∏–¥–µ–∞–ª—å–Ω—ã–π DCG)
        ideal_scores = sorted(relevance_scores, reverse=True)
        idcg = 0.0
        for i, score in enumerate(ideal_scores[:k]):
            if score > 0:
                idcg += score / math.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def calculate_mrr(self, first_relevant_positions: List[int]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç Mean Reciprocal Rank"""
        if not first_relevant_positions:
            return 0.0
        
        reciprocal_ranks = []
        for pos in first_relevant_positions:
            if pos > 0:
                reciprocal_ranks.append(1.0 / pos)
            else:
                reciprocal_ranks.append(0.0)
        
        return sum(reciprocal_ranks) / len(reciprocal_ranks)
    
    def calculate_map(self, relevant_items_per_query: List[List[bool]]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç Mean Average Precision"""
        if not relevant_items_per_query:
            return 0.0
        
        average_precisions = []
        for relevant_items in relevant_items_per_query:
            if not any(relevant_items):
                average_precisions.append(0.0)
                continue
            
            precisions = []
            relevant_count = 0
            
            for i, is_relevant in enumerate(relevant_items):
                if is_relevant:
                    relevant_count += 1
                    precision = relevant_count / (i + 1)
                    precisions.append(precision)
            
            if precisions:
                average_precisions.append(sum(precisions) / len(precisions))
            else:
                average_precisions.append(0.0)
        
        return sum(average_precisions) / len(average_precisions)
    
    def calculate_title_match_score(self, results: List[Dict], expected_titles: List[str]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –≤ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö"""
        if not expected_titles or not results:
            return 0.0
        
        title_scores = []
        for expected in expected_titles:
            best_score = 0.0
            expected_words = set(expected.lower().split())
            
            for i, result in enumerate(results[:10]):  # –°–º–æ—Ç—Ä–∏–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-10
                title = result.get('title', '').lower()
                book_name = result.get('book_name', '').lower()
                
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if expected.lower() in title or expected.lower() in book_name:
                    score = 3.0 / (i + 1)  # –ë—É—Å—Ç –∑–∞ –ø–æ–∑–∏—Ü–∏—é
                # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞–º
                elif expected_words.intersection(set(title.split() + book_name.split())):
                    word_overlap = len(expected_words.intersection(set(title.split() + book_name.split())))
                    score = (word_overlap / len(expected_words)) * 2.0 / (i + 1)
                else:
                    score = 0.0
                
                best_score = max(best_score, score)
            
            title_scores.append(best_score)
        
        return sum(title_scores) / len(title_scores) if title_scores else 0.0
    
    def calculate_diversity_score(self, results: List[Dict]) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not results:
            return 0.0
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ path_index (—Ç–∏–ø—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞)
        path_counts = defaultdict(int)
        for result in results[:20]:  # –°–º–æ—Ç—Ä–∏–º —Ç–æ–ø-20
            path_counts[result.get('path_index', 'unknown')] += 1
        
        # –í—ã—á–∏—Å–ª—è–µ–º —ç–Ω—Ç—Ä–æ–ø–∏—é –®–µ–Ω–Ω–æ–Ω–∞
        total = sum(path_counts.values())
        if total <= 1:
            return 0.0
        
        entropy = 0.0
        for count in path_counts.values():
            if count > 0:
                p = count / total
                entropy -= p * math.log2(p)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ [0, 1]
        max_entropy = math.log2(min(len(path_counts), total))
        return entropy / max_entropy if max_entropy > 0 else 0.0
    
    def calculate_speed_score(self, execution_time: float) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–∫–æ—Ä–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞"""
        # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —à–∫–∞–ª–∞: –±—ã—Å—Ç—Ä—ã–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—É—á–∞—é—Ç –≤—ã—Å–æ–∫–∏–π —Å–∫–æ—Ä
        if execution_time <= 0:
            return 1.0
        
        # –ò–¥–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è: 0.5—Å = 1.0, 2—Å = 0.5, 5—Å = 0.2
        return max(0.0, min(1.0, 1.0 / (1.0 + execution_time)))
    
    def evaluate_search_quality(self, 
                               query: str,
                               results: List[Dict], 
                               expected_titles: List[str],
                               execution_time: float,
                               category: str = "general") -> SearchMetrics:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ–∏—Å–∫–∞"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        relevance_scores = []
        relevant_items = []
        
        for i, result in enumerate(results[:20]):  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø-20
            title = result.get('title', '').lower()
            book_name = result.get('book_name', '').lower()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            relevance = 0
            is_relevant = False
            
            for expected in expected_titles:
                expected_lower = expected.lower()
                expected_words = set(expected_lower.split())
                result_words = set(title.split() + book_name.split())
                
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if expected_lower in title or expected_lower in book_name:
                    relevance = max(relevance, 3)
                    is_relevant = True
                # –í—ã—Å–æ–∫–æ–µ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                elif len(expected_words.intersection(result_words)) >= len(expected_words) * 0.8:
                    relevance = max(relevance, 2)
                    is_relevant = True
                # –°—Ä–µ–¥–Ω–µ–µ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                elif len(expected_words.intersection(result_words)) >= len(expected_words) * 0.5:
                    relevance = max(relevance, 1)
                    is_relevant = True
            
            relevance_scores.append(relevance)
            relevant_items.append(is_relevant)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö K
        k_values = [1, 3, 5, 10, 20]
        precision_at_k = {}
        recall_at_k = {}
        ndcg_at_k = {}
        
        total_relevant = len(expected_titles)
        
        for k in k_values:
            precision_at_k[k] = self.calculate_precision_at_k(relevant_items, k)
            recall_at_k[k] = self.calculate_recall_at_k(relevant_items, total_relevant, k)
            ndcg_at_k[k] = self.calculate_ndcg_at_k(relevance_scores, k)
        
        # MRR - –ø–æ–∑–∏—Ü–∏—è –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        first_relevant_pos = 0
        for i, is_relevant in enumerate(relevant_items):
            if is_relevant:
                first_relevant_pos = i + 1
                break
        
        mrr = 1.0 / first_relevant_pos if first_relevant_pos > 0 else 0.0
        
        # MAP - –¥–ª—è –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ AP
        map_score = self.calculate_map([relevant_items])
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        title_match_score = self.calculate_title_match_score(results, expected_titles)
        diversity_score = self.calculate_diversity_score(results)
        speed_score = self.calculate_speed_score(execution_time)
        
        return SearchMetrics(
            precision_at_k=precision_at_k,
            recall_at_k=recall_at_k,
            ndcg_at_k=ndcg_at_k,
            mrr=mrr,
            map_score=map_score,
            title_match_score=title_match_score,
            diversity_score=diversity_score,
            speed_score=speed_score,
            category_scores={category: (precision_at_k[5] + ndcg_at_k[5]) / 2}
        )
    
    def aggregate_metrics(self, metrics_list: List[SearchMetrics]) -> SearchMetrics:
        """–ê–≥—Ä–µ–≥–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤—É –∑–∞–ø—Ä–æ—Å–æ–≤"""
        if not metrics_list:
            return SearchMetrics(
                precision_at_k={}, recall_at_k={}, ndcg_at_k={},
                mrr=0.0, map_score=0.0, title_match_score=0.0,
                diversity_score=0.0, speed_score=0.0, category_scores={}
            )
        
        n = len(metrics_list)
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º precision@k
        k_values = list(metrics_list[0].precision_at_k.keys())
        precision_at_k = {}
        recall_at_k = {}
        ndcg_at_k = {}
        
        for k in k_values:
            precision_at_k[k] = sum(m.precision_at_k[k] for m in metrics_list) / n
            recall_at_k[k] = sum(m.recall_at_k[k] for m in metrics_list) / n
            ndcg_at_k[k] = sum(m.ndcg_at_k[k] for m in metrics_list) / n
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        mrr = sum(m.mrr for m in metrics_list) / n
        map_score = sum(m.map_score for m in metrics_list) / n
        title_match_score = sum(m.title_match_score for m in metrics_list) / n
        diversity_score = sum(m.diversity_score for m in metrics_list) / n
        speed_score = sum(m.speed_score for m in metrics_list) / n
        
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        category_scores = defaultdict(list)
        for m in metrics_list:
            for category, score in m.category_scores.items():
                category_scores[category].append(score)
        
        aggregated_category_scores = {}
        for category, scores in category_scores.items():
            aggregated_category_scores[category] = sum(scores) / len(scores)
        
        return SearchMetrics(
            precision_at_k=precision_at_k,
            recall_at_k=recall_at_k,
            ndcg_at_k=ndcg_at_k,
            mrr=mrr,
            map_score=map_score,
            title_match_score=title_match_score,
            diversity_score=diversity_score,
            speed_score=speed_score,
            category_scores=aggregated_category_scores
        )
    
    def calculate_composite_score(self, metrics: SearchMetrics) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π —Å–∫–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞"""
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        weights = {
            'precision': 0.25,
            'ndcg': 0.25,
            'title_match': 0.20,
            'mrr': 0.15,
            'diversity': 0.10,
            'speed': 0.05
        }
        
        score = (
            weights['precision'] * metrics.precision_at_k.get(5, 0.0) * 100 +
            weights['ndcg'] * metrics.ndcg_at_k.get(5, 0.0) * 100 +
            weights['title_match'] * metrics.title_match_score * 100 +
            weights['mrr'] * metrics.mrr * 100 +
            weights['diversity'] * metrics.diversity_score * 100 +
            weights['speed'] * metrics.speed_score * 100
        )
        
        return min(100.0, score)

def format_metrics_report(metrics: SearchMetrics, query: str = "") -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º"""
    report = []
    
    if query:
        report.append(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: '{query}'")
        report.append("=" * 50)
    
    # Precision@K
    report.append("üéØ Precision@K:")
    for k, score in sorted(metrics.precision_at_k.items()):
        report.append(f"   P@{k}: {score:.3f}")
    
    # NDCG@K
    report.append("\nüèÜ NDCG@K:")
    for k, score in sorted(metrics.ndcg_at_k.items()):
        report.append(f"   NDCG@{k}: {score:.3f}")
    
    # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    report.append(f"\n‚ö° MRR: {metrics.mrr:.3f}")
    report.append(f"üìà MAP: {metrics.map_score:.3f}")
    report.append(f"üìù Title Match: {metrics.title_match_score:.3f}")
    report.append(f"üé® Diversity: {metrics.diversity_score:.3f}")
    report.append(f"üöÄ Speed: {metrics.speed_score:.3f}")
    
    if metrics.category_scores:
        report.append("\nüìÇ –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
        for category, score in metrics.category_scores.items():
            report.append(f"   {category}: {score:.3f}")
    
    return "\n".join(report)
